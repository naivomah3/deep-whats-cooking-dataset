{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural-Net for Tabular data\n",
    "### <u>Problem statement 5:</u> \n",
    "The following dataset is taken from kaggle [What's Cooking Dataset](https://www.kaggle.com/c/whats-cooking-kernels-only/data). \n",
    "\n",
    "The dataset is a list of ingredients and the model is expected to classify the cuisine based on the given set of ingredients. This problem can be framed as multi class classification and having only 1 independent variable `ingredients`.<br> <br>\n",
    "On the other hand, this variable holds a number of ingredients of which can be considered as features/categories. As each observation is typically different from one another, this variable carries on high cardinality.<br>\n",
    "The problem will be addressed using CNN as one variable detains several features which must be captured first, then aggregate afterward <br>\n",
    "The steps would be as follow: <br>\n",
    "* Encode each category within the `ingredients` variable \n",
    "* Embed them all to have a fixed-size vector representation \n",
    "* Capture features of each embedded vector of `ingredients`\n",
    "* Wrap up all captured features into 2-dimension which represents each observation\n",
    "* Classify this embedded & wrapped feature using dense layer as a classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Print out all rows and cols\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_flat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "      <td>romaine lettuce black olives grape tomatoes ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "      <td>plain flour ground pepper salt tomatoes ground...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "      <td>eggs pepper salt mayonaise cooking oil green c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "      <td>water vegetable oil wheat salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "      <td>black pepper shallots cornflour cayenne pepper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients  \\\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...   \n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...   \n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...   \n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]   \n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe...   \n",
       "\n",
       "                                    ingredients_flat  \n",
       "0  romaine lettuce black olives grape tomatoes ga...  \n",
       "1  plain flour ground pepper salt tomatoes ground...  \n",
       "2  eggs pepper salt mayonaise cooking oil green c...  \n",
       "3                     water vegetable oil wheat salt  \n",
       "4  black pepper shallots cornflour cayenne pepper...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recipes = pd.read_json('datasets/train.json')\n",
    "df_recipes[\"ingredients_flat\"] = df_recipes[\"ingredients\"].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df_recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Number & List of classes:\n",
      " 20/['brazilian' 'british' 'cajun_creole' 'chinese' 'filipino' 'french'\n",
      " 'greek' 'indian' 'irish' 'italian' 'jamaican' 'japanese' 'korean'\n",
      " 'mexican' 'moroccan' 'russian' 'southern_us' 'spanish' 'thai'\n",
      " 'vietnamese']\n",
      "================================================================================\n",
      "Target shape after encoding(OHE): \n",
      " (39774, 20)\n"
     ]
    }
   ],
   "source": [
    "# Encode the target variable \n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "enc = preprocessing.LabelEncoder()\n",
    "enc.fit(df_recipes[\"cuisine\"].values)\n",
    "\n",
    "targets_enc = enc.transform(df_recipes[\"cuisine\"].values)\n",
    "# OneHot the target ->  \n",
    "# so that we can make use of `categorical_crossentrpy` as loss function \n",
    "# and `softmax` as predictor/activation at the output layer\n",
    "targets = to_categorical(targets_enc)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Number & List of classes:\\n {len(enc.classes_)}/{enc.classes_}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Target shape after encoding(OHE): \\n {targets.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximun number of ingredients in the recipes:\n",
      " 65\n",
      "Manimun number of ingredients in the recipes:\n",
      " 1\n",
      "================================================================================\n",
      "Final scaled 'ingredients length' variable shape:\n",
      " (39774, 1)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zero/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/home/zero/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a new feature(may be interested): the number of ingredients in the recipe\n",
    "df_recipes[\"ingredients_len\"] = df_recipes[\"ingredients\"].apply(lambda x: len(x))\n",
    "ingredients_len = df_recipes[[\"ingredients_len\"]].values \n",
    "\n",
    "print(f\"Maximun number of ingredients in the documents(recipes):\\n {ingredients_len.max()}\")\n",
    "print(f\"Manimun number of ingredients in the documents(recipes):\\n {ingredients_len.min()}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Scale the new feature \n",
    "scaler = preprocessing.StandardScaler()\n",
    "ingredients_len_scaled = scaler.fit_transform(ingredients_len)\n",
    "print(f\"Final scaled 'ingredients length' variable shape:\\n {ingredients_len_scaled.shape}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few sample as array of strings:\n",
      " ['romaine lettuce black olives grape tomatoes garlic pepper purple onion seasoning garbanzo beans feta cheese crumbles'\n",
      " 'plain flour ground pepper salt tomatoes ground black pepper thyme eggs green tomatoes yellow corn meal milk vegetable oil']\n",
      "================================================================================\n",
      "Number of tokens in the documents:\n",
      " 3065\n",
      "================================================================================\n",
      "Few sample as array of integer:\n",
      " [[314, 138, 13, 128, 339, 18, 4, 1, 104, 25, 79, 489, 50, 204, 10, 287]]\n",
      "================================================================================\n",
      "Final 'ingredients' variable shape after padding:\n",
      " (39774, 40)\n",
      "================================================================================\n",
      "Sample of 'ingredients' variable representation after padding:\n",
      " [[314 138  13 128 339  18   4   1 104  25  79 489  50 204  10 287   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Get list of ingredients of all rows to numpy array \n",
    "ingredients_docs = df_recipes[\"ingredients_flat\"].values\n",
    "print(f\"Few sample as array of strings:\\n {ingredients_docs[:2]}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tokenize all the words: just to count tokens \n",
    "# in order to get the maximum number of words in any documents\n",
    "# and the oveal number of words in all corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(ingredients_docs)\n",
    "\n",
    "# Vocab size is used to encode each and every word in the document to and unique integer \n",
    "ingredients_docs_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocab size of the documents:\\n {ingredients_docs_vocab_size}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# Encode/Transform rows to integer after tokenization\n",
    "ingredients_docs_enc = tokenizer.texts_to_sequences(ingredients_docs)\n",
    "print(f\"Few sample as array of integer:\\n {ingredients_docs_enc[:1]}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Narrow the size of ingredients vector of each doc into 40 and pad them if less than 40\n",
    "# As we can see earlier the maximum number of ingredients is 65 and the minimum is 1\n",
    "# This process will take into account the 40 first ingredients in case the doc size(# of words) is more than 40. \n",
    "# This number is an hyper-parameter that we can tune - later on\n",
    "max_length = 40\n",
    "ingredients_docs_padded = pad_sequences(ingredients_docs_enc, \n",
    "                                        maxlen=max_length, \n",
    "                                        padding='post', \n",
    "                                        values=0.0, # Zero padding\n",
    "                                       )\n",
    "\n",
    "print(f\"Final 'ingredients' variable shape after Padding:\\n {ingredients_docs_padded.shape}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Sample of 'ingredients' variable representation after padding:\\n {ingredients_docs_padded[:1]}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(ingredients_docs_padded.shape[0], batch_size)\n",
    "        \n",
    "        yield ({'cat_inputs': ingredients_docs_padded[batch_idx],\n",
    "                'num_inputs': ingredients_len_scaled[batch_idx],}, \n",
    "               {'output': targets[batch_idx]}\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule of thumb: size of embedding vector w.r.t the number of categories \n",
    "def embedding_size(no_cat):\n",
    "    return min(600, round(1.6 * no_cat**0.56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Input, Dropout, Dense, \n",
    "                                     BatchNormalization, Embedding, \n",
    "                                     Flatten, Concatenate, Conv1D, \n",
    "                                     Activation, GlobalAveragePooling1D,\n",
    "                                     GlobalMaxPool1D, RepeatVector\n",
    "                                    )\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Dropout probability \n",
    "p = .1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inputs = Input(shape=(40,), name=\"cat_inputs\")\n",
    "num_inputs = Input(shape=(1,), name=\"num_inputs\")\n",
    "\n",
    "#==== Categorical Transformation ====\n",
    "# Encode the category using embedding layer\n",
    "# Embedding layer weights are trainable \n",
    "embedding_layer = Embedding(input_dim=ingredients_docs_vocab_size, # number of unique words in the vocab\n",
    "                            output_dim=embedding_size(ingredients_docs_vocab_size),\n",
    "                            input_length=40\n",
    "                           )\n",
    "\n",
    "X_cat = embedding_layer(cat_inputs)\n",
    "\n",
    "global_avg = GlobalAveragePooling1D(name='global_avg_1')(X_cat)\n",
    "global_max = GlobalMaxPool1D(name='global_max_1')(X_cat)\n",
    "x = Concatenate()([global_avg, global_max])\n",
    "\n",
    "# just repeat the previous layer 40 times to match its dimension with the \n",
    "# to the dimension of the input layer \n",
    "x = RepeatVector(40, name='repeat')(x) \n",
    "x = Concatenate()([X_cat, x])\n",
    "\n",
    "x = Dropout(p)(x)\n",
    "x = Conv1D(20, 1)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "global_avg = GlobalAveragePooling1D(name='global_avg_2')(x)\n",
    "global_max = GlobalMaxPool1D(name='global_max_2')(x)\n",
    "x = Concatenate()([global_avg, global_max])\n",
    "\n",
    "#==== Append the categorical transformed features to the numerical features ====\n",
    "x = Concatenate()([x, num_inputs])\n",
    "#===== \n",
    "x = Dropout(p)(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "#===== \n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "#===== \n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(10, activation='relu')(x)\n",
    "#===== \n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "#=====\n",
    "out = Dense(20, activation='softmax', name='output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 40)]\n",
      "(None, 40, 143)\n",
      "(None, 143)\n",
      "(None, 143)\n",
      "(None, 286)\n",
      "(None, 40, 286)\n",
      "(None, 40, 429)\n",
      "(None, 40, 429)\n",
      "(None, 40, 20)\n",
      "(None, 40, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 40)\n",
      "[(None, 1)]\n",
      "(None, 41)\n",
      "(None, 41)\n",
      "(None, 100)\n",
      "(None, 100)\n",
      "(None, 100)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 10)\n",
      "(None, 10)\n",
      "(None, 10)\n",
      "(None, 20)\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[cat_inputs, num_inputs], outputs=out)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "cat_inputs (InputLayer)         [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 143)      438295      cat_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_avg_1 (GlobalAveragePool (None, 143)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_1 (GlobalMaxPooling1 (None, 143)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 286)          0           global_avg_1[0][0]               \n",
      "                                                                 global_max_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "repeat (RepeatVector)           (None, 40, 286)      0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 40, 429)      0           embedding_1[0][0]                \n",
      "                                                                 repeat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 40, 429)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 40, 20)       8600        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 40, 20)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_avg_2 (GlobalAveragePool (None, 20)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_2 (GlobalMaxPooling1 (None, 20)           0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 40)           0           global_avg_2[0][0]               \n",
      "                                                                 global_max_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "num_inputs (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 41)           0           concatenate_6[0][0]              \n",
      "                                                                 num_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 41)           0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          4200        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_3 (Batch (None, 100)          400         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 100)          0           batch_normalization_v2_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           2020        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_4 (Batch (None, 20)           80          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20)           0           batch_normalization_v2_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 10)           210         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_5 (Batch (None, 10)           40          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 10)           0           batch_normalization_v2_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 20)           220         dropout_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 454,065\n",
      "Trainable params: 453,805\n",
      "Non-trainable params: 260\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy', \n",
    "             metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/312 [==============================] - 7s 23ms/step - loss: 2.4869 - accuracy: 0.3058\n",
      "Epoch 2/20\n",
      "313/312 [==============================] - 4s 11ms/step - loss: 1.7908 - accuracy: 0.5004\n",
      "Epoch 3/20\n",
      "313/312 [==============================] - 4s 12ms/step - loss: 1.5216 - accuracy: 0.5721\n",
      "Epoch 4/20\n",
      "313/312 [==============================] - 4s 11ms/step - loss: 1.3886 - accuracy: 0.6138\n",
      "Epoch 5/20\n",
      "313/312 [==============================] - 4s 12ms/step - loss: 1.3039 - accuracy: 0.6360\n",
      "Epoch 6/20\n",
      "313/312 [==============================] - 4s 14ms/step - loss: 1.2302 - accuracy: 0.6531\n",
      "Epoch 7/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 1.1792 - accuracy: 0.6668\n",
      "Epoch 8/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 1.1459 - accuracy: 0.6773\n",
      "Epoch 9/20\n",
      "313/312 [==============================] - 5s 15ms/step - loss: 1.0864 - accuracy: 0.6847\n",
      "Epoch 10/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 1.0892 - accuracy: 0.6939\n",
      "Epoch 11/20\n",
      "313/312 [==============================] - 5s 17ms/step - loss: 1.0554 - accuracy: 0.7015\n",
      "Epoch 12/20\n",
      "313/312 [==============================] - 5s 17ms/step - loss: 1.0119 - accuracy: 0.7075\n",
      "Epoch 13/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 0.9960 - accuracy: 0.7194\n",
      "Epoch 14/20\n",
      "313/312 [==============================] - 5s 17ms/step - loss: 0.9699 - accuracy: 0.7251\n",
      "Epoch 15/20\n",
      "313/312 [==============================] - 5s 15ms/step - loss: 0.9937 - accuracy: 0.7200\n",
      "Epoch 16/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 0.9472 - accuracy: 0.7335\n",
      "Epoch 17/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 0.9449 - accuracy: 0.7347\n",
      "Epoch 18/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 0.9283 - accuracy: 0.7342\n",
      "Epoch 19/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 0.9105 - accuracy: 0.7405\n",
      "Epoch 20/20\n",
      "313/312 [==============================] - 5s 16ms/step - loss: 0.9054 - accuracy: 0.7489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc4c4fbf310>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 / batch_size,\n",
    "    epochs=20,\n",
    "    max_queue_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep37",
   "language": "python",
   "name": "deep37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
