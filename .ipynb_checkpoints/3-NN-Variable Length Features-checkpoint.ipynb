{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural-Net for Tabular data\n",
    "### <u>Problem statement 3:</u> Variable length features\n",
    "The following dataset is a synthetic dataset generated from `make_classification`. <br>\n",
    "One variable withholds some set of features. How can can we consider those features? In the following, we assume there are 10 different types of credit card and each credit card has features like `Valid_Date`, `Daily_Max_Transaction`, etc...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = make_classification(\n",
    "    n_samples=10_000, \n",
    "    n_features=30,\n",
    "    n_clusters_per_class=2,\n",
    "    n_informative=10,\n",
    "    n_classes=4\n",
    ")\n",
    "X, y = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classes = list()\n",
    "\n",
    "for i in range(4):\n",
    "    base_classes.append(X_scaled[y == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_points = 5_000\n",
    "class1_dist = [.5, .5, 0, 0]\n",
    "class2_dist = [0, .2, .6, .2]\n",
    "\n",
    "def make_var_len_feature_point(dist):\n",
    "    feature_sets = []\n",
    "    num_features = np.random.randint(3, 11)\n",
    "    for _ in range(num_features):\n",
    "        # choose which distribution the credit card comes from\n",
    "        base_class = np.random.choice([0, 1, 2, 3], 1, p=dist)\n",
    "        base_class_points = base_classes[base_class[0]]\n",
    "        feature_set_idx = np.random.choice(base_class_points.shape[0], 1)\n",
    "        feature_sets.append(base_class_points[feature_set_idx])\n",
    "        \n",
    "    for _ in range(10 - num_features):\n",
    "        feature_sets.append(np.zeros((1, 30)))\n",
    "\n",
    "    return np.concatenate(feature_sets)[np.newaxis, :, :]\n",
    "\n",
    "\n",
    "class1_points = []\n",
    "for _ in range(num_points):\n",
    "    class1_points.append(\n",
    "        make_var_len_feature_point(class1_dist)\n",
    "    )\n",
    "class1_points = np.concatenate(class1_points)\n",
    "    \n",
    "class2_points = []\n",
    "for _ in range(num_points):\n",
    "    class2_points.append(\n",
    "        make_var_len_feature_point(class2_dist)\n",
    "    )\n",
    "class2_points = np.concatenate(class2_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class2_points.shape # (no_people, no_credit_cards, no_credit_cards_features{validty, end_date, ...})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generator(batch_size):\n",
    "    while True:\n",
    "        batch_idx = np.random.choice(\n",
    "            class1_points.shape[0], batch_size // 2)\n",
    "        batch_x = np.concatenate([\n",
    "            class1_points[batch_idx],\n",
    "            class2_points[batch_idx],\n",
    "        ])\n",
    "        batch_y = np.concatenate([\n",
    "            np.zeros(batch_size // 2),\n",
    "            np.ones(batch_size // 2),\n",
    "        ])\n",
    "        yield ({'no_inputs': batch_x}, \n",
    "               {'output': batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Input, Dropout, Dense, \n",
    "                                     BatchNormalization, Embedding, \n",
    "                                     Flatten, Concatenate, Conv1D, \n",
    "                                     Activation, GlobalAveragePooling1D,\n",
    "                                     GlobalMaxPool1D, RepeatVector\n",
    "                                    )\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Dropout probability \n",
    "p = .1\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The use of Convolution network is important cause with it we can extract important features \n",
    "# by applying the same function to each credit card\n",
    "# Use Convolution Layer to treat each feature set of each Credit Card separatly\n",
    "# Use GlobalMax/AveragePool to combine information from all the cards together into 1\n",
    "\n",
    "#------ Get features from each credit card -------\n",
    "inputs = Input((10, 30), name='no_inputs') # Eg. 10 credit cards records having 30 features each  \n",
    "\n",
    "\n",
    "x = Dropout(p)(inputs)\n",
    "x = Conv1D(10, 1)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "global_ave = GlobalAveragePooling1D()(x)\n",
    "global_max = GlobalMaxPool1D()(x)\n",
    "\n",
    "x = Concatenate()([global_ave, global_max])\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "#------- SKIP-CONNECTION(Concatenation): appending the global context  \n",
    "#------- Combine the extracted features which held the global information\n",
    "#------- of the 10 different credit cards records to the inputs \n",
    "x = RepeatVector(10)(x)\n",
    "x = Concatenate()([inputs, x])\n",
    "\n",
    "x = Dropout(p)(x)\n",
    "x = Conv1D(10, 1)(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "global_ave = GlobalAveragePooling1D()(x)\n",
    "global_max = GlobalMaxPool1D()(x)\n",
    "x = Concatenate()([global_ave, global_max])\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "#---- Dense Layers \n",
    "x = Dropout(p)(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(20, activation='relu')(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "x = Dense(10, activation='relu')(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(p)(x)\n",
    "out = Dense(1, activation='sigmoid', name='output')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 10, 30)]\n",
      "(None, 10, 30)\n",
      "(None, 10, 10)\n",
      "(None, 10, 10)\n",
      "(None, 10)\n",
      "(None, 10)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 10, 20)\n",
      "(None, 10, 50)\n",
      "(None, 10, 50)\n",
      "(None, 10, 10)\n",
      "(None, 10, 10)\n",
      "(None, 10)\n",
      "(None, 10)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 100)\n",
      "(None, 100)\n",
      "(None, 100)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 20)\n",
      "(None, 10)\n",
      "(None, 10)\n",
      "(None, 10)\n",
      "(None, 1)\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy', \n",
    "             metrics=['accuracy'])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/312 [==============================] - 6s 20ms/step - loss: 0.5717 - accuracy: 0.6923\n",
      "Epoch 2/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.3982 - accuracy: 0.8207\n",
      "Epoch 3/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.3202 - accuracy: 0.8615\n",
      "Epoch 4/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.2770 - accuracy: 0.8821\n",
      "Epoch 5/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.2401 - accuracy: 0.9006\n",
      "Epoch 6/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.2050 - accuracy: 0.9161\n",
      "Epoch 7/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1871 - accuracy: 0.9261\n",
      "Epoch 8/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1709 - accuracy: 0.9335\n",
      "Epoch 9/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1612 - accuracy: 0.9370\n",
      "Epoch 10/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1545 - accuracy: 0.9372\n",
      "Epoch 11/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1511 - accuracy: 0.9412\n",
      "Epoch 12/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1503 - accuracy: 0.9398\n",
      "Epoch 13/20\n",
      "313/312 [==============================] - 2s 6ms/step - loss: 0.1371 - accuracy: 0.9447\n",
      "Epoch 14/20\n",
      "313/312 [==============================] - 2s 6ms/step - loss: 0.1395 - accuracy: 0.9453\n",
      "Epoch 15/20\n",
      "313/312 [==============================] - 2s 7ms/step - loss: 0.1385 - accuracy: 0.9452\n",
      "Epoch 16/20\n",
      "313/312 [==============================] - 2s 7ms/step - loss: 0.1309 - accuracy: 0.9485\n",
      "Epoch 17/20\n",
      "313/312 [==============================] - 2s 6ms/step - loss: 0.1306 - accuracy: 0.9488\n",
      "Epoch 18/20\n",
      "313/312 [==============================] - 2s 6ms/step - loss: 0.1281 - accuracy: 0.9508\n",
      "Epoch 19/20\n",
      "313/312 [==============================] - 2s 5ms/step - loss: 0.1268 - accuracy: 0.9505\n",
      "Epoch 20/20\n",
      "313/312 [==============================] - 2s 6ms/step - loss: 0.1207 - accuracy: 0.9551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7968303c90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    sample_generator(batch_size),\n",
    "    steps_per_epoch=10_000 / batch_size,\n",
    "    epochs=20,\n",
    "    max_queue_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep37",
   "language": "python",
   "name": "deep37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
